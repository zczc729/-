{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e543896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ChatMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"You always answer into Korean. You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n",
    "\n",
    "SYS_PROMPT_TEMPLATE = \"You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability. You always answer succinctly. You must always answer in Korean.\"\n",
    "\n",
    "class MyEmbeddings(Embeddings):\n",
    "    def __init__(self, base_url, api_key=\"lm-studio\"):\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "    def embed_documents(self, texts: List[str], model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\") -> List[List[float]]:\n",
    "        texts = list(map(lambda text:text.replace(\"\\n\", \" \"), texts))\n",
    "        datas = self.client.embeddings.create(input=texts, model=model).data\n",
    "        return list(map(lambda data:data.embedding, datas))\n",
    "        \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# 언어모델\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    temperature=0.1,\n",
    "    )\n",
    "\n",
    "# 임베딩 모델\n",
    "emb = MyEmbeddings(base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"], length_function=len,\n",
    ")\n",
    "\n",
    "# RAG 체인\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You always answer into Korean.\"),\n",
    "    (\"user\", RAG_PROMPT_TEMPLATE),\n",
    "])\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 일반 채팅 체인\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYS_PROMPT_TEMPLATE),\n",
    "    MessagesPlaceholder(variable_name='messsages1'),\n",
    "])\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 파일 -> 벡터저장소\n",
    "def embed_file(file_path):\n",
    "    # 문서 불러오고 분할\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    Loader = {'txt': TextLoader, 'pdf': PyPDFLoader}[file_extension]\n",
    "    docs = Loader(file_path).load_and_split(text_splitter=text_splitter)\n",
    "    # 벡터화\n",
    "    vectorstore = FAISS.from_documents(docs, embedding=emb, distance_strategy=DistanceStrategy.COSINE)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k':10})\n",
    "    return retriever\n",
    "\n",
    "# 메인 실행 부분\n",
    "messages = [\n",
    "    ChatMessage(role=\"assistant\", content=\"앞에 '!'(느낌표)를 붙이면 문서검색 후 답변합니다.\"),\n",
    "]\n",
    "\n",
    "# 파일 임베딩 (예: 'sample.txt' 파일을 사용)\n",
    "file_path = 'sample.txt'  # 실제 파일 경로로 변경해주세요\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    retriever = embed_file(file_path)\n",
    "    print(f\"File '{file_path}' has been embedded.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found. Proceeding without document retrieval.\")\n",
    "    retriever = None\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    messages.append(ChatMessage(role='user', content=user_input))\n",
    "    retrieve_flag = user_input[0] == '!'\n",
    "    if retrieve_flag:\n",
    "        user_input = user_input[1:]\n",
    "\n",
    "    if retriever and retrieve_flag:\n",
    "        format_docs = lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | rag_chain\n",
    "        )\n",
    "        response = chain.invoke(user_input)\n",
    "    else:\n",
    "        response = chat_chain.invoke(messages)\n",
    "\n",
    "    print(\"Assistant:\", response)\n",
    "    messages.append(ChatMessage(role='assistant', content=response))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
