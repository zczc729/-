{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 운영체제 확인\n",
    "import os\n",
    "os.name\n",
    "\n",
    "# 운영체제별 한글 폰트 설정\n",
    "if os.name == 'posix': # Mac 환경 폰트 설정\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif os.name == 'nt': # Windows 환경 폰트 설정\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "plt.rc('axes', unicode_minus=False) # 마이너스 폰트 설정\n",
    "\n",
    "\n",
    "# 글씨 선명하게 출력하는 설정\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data_dacon/train.csv\", parse_dates=['일자'])\n",
    "test = pd.read_csv(\"data_dacon/test.csv\", parse_dates=['일자'])\n",
    "submission = pd.read_csv(\"data_dacon/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year'] = train['일자'].dt.year\n",
    "train['month'] = train['일자'].dt.month\n",
    "train['day']  = train['일자'].dt.day\n",
    "train['dayofweek'] = train['일자'].dt.day_of_week\n",
    "\n",
    "test['year'] = test['일자'].dt.year\n",
    "test['month'] = test['일자'].dt.month\n",
    "test['day']  = test['일자'].dt.day\n",
    "test['dayofweek'] = test['일자'].dt.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['출근자수'] = train['본사정원수'] - train['본사휴가자수'] - train['본사출장자수'] - train['현본사소속재택근무자수']\n",
    "test['출근자수'] = test['본사정원수'] - test['본사휴가자수'] - test['본사출장자수'] - test['현본사소속재택근무자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['식사자수'] = train['출근자수'] * (1 + train['본사시간외근무명령서승인건수'] / train['출근자수'])\n",
    "test['식사자수'] = test['출근자수'] * (1 + test['본사시간외근무명령서승인건수'] / test['출근자수'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[0]['중식계']/ train.iloc[0]['출근자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[0]['석식계']/ train.iloc[0]['출근자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['중식비율'] = train['중식계'] / train['출근자수']\n",
    "train['석식비율'] = train['석식계'] / train['출근자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['출근자수', '중식비율', '석식비율', '중식계', '석식계']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train['석식계'] == 0) & (train['요일'] =='화')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train['중식메뉴'].iloc[0].split()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.index('쌀밥/잡곡밥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_lunch(lunch):\n",
    "    tmp = lunch.split()\n",
    "    for menu in tmp:\n",
    "        if \"(\" in menu:\n",
    "            tmp.remove(menu)\n",
    "        if \"쌀밥\" in menu:\n",
    "            test1 = tmp.index(menu)\n",
    "            tmp[test1] = \"밥\"\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [밥, 오징어찌개, 쇠불고기, 계란찜, 청포묵무침, 요구르트, 포기김치]\n",
       "1           [밥, 김치찌개, 가자미튀김, 모둠소세지구이, 마늘쫑무침, 요구르트, 배추겉절이]\n",
       "2           [카레덮밥, 팽이장국, 치킨핑거, 쫄면야채무침, 견과류조림, 요구르트, 포기김치]\n",
       "3               [밥, 쇠고기무국, 주꾸미볶음, 부추전, 시금치나물, 요구르트, 포기김치]\n",
       "4                [밥, 떡국, 돈육씨앗강정, 우엉잡채, 청경채무침, 요구르트, 포기김치]\n",
       "                              ...                        \n",
       "1200    [밥, 아욱국, 수제함박스테이크, 견과류마카로니범벅, 생깻잎지, 단호박물김치, 양상...\n",
       "1201    [밥, 냉이된장국, 동파육, 봄동전, 청경채/버섯숙회*초장, 무생채, 양상추샐러드*...\n",
       "1202    [전주비빔밥*약고추장, 계란파국, 요거닭, 올방개묵무침, 파프리카해초무침, 포기김치...\n",
       "1203    [밥, 전주식콩나물해장국, 돈육간장불고기, 깐풍연근, 연두부*달래양념장, 봄동겉절이...\n",
       "1204    [밥, 들깨미역국, 교촌간장치킨, 옥수수콘치즈구이, 가지고추장무침, 포기김치/요구르...\n",
       "Name: lunch_menu, Length: 1205, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lunch_menu'] = train['중식메뉴'].apply(sep_lunch)\n",
    "train['lunch_menu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [밥, 대구지리, 매운돈갈비찜, 오꼬노미계란말이, 상추무침, 포기김치, 양상추샐러드...\n",
       "1     [밥, 우렁된장찌개, 오리주물럭, 청양부추전, 수제삼색무쌈, 겉절이김치, 양상추샐러...\n",
       "2     [밥, 팽이장국, 수제돈까스*소스, 가자미조림, 동초나물무침, 포기김치, 양상추샐러...\n",
       "3     [밥, 배추들깨국, 오리대패불고기, 시금치프리타타, 부추고추장무침, 포기김치, 양상...\n",
       "4     [밥, 부대찌개, 닭살데리야끼조림, 버섯탕수, 세발나물무침, 알타리김치/사과푸딩, ...\n",
       "5         [밥, 아욱국, 매콤해물볶음, 감자조림, 미나리나물, 포기김치, 콥샐러드*렌치D]\n",
       "6     [밥, 설렁탕, 고등어김치말이찜, 볼어묵굴소스볶음, 브로콜리숙회*초장, 석박지, 양...\n",
       "7     [밥, 북엇국, 닭볶음탕, 채소전*장, 솎음열무나물무침, 포기김치, 양상추샐러드*황도D]\n",
       "8     [밥, 감자양파국, 돈수육*씨앗쌈장, 매콤어묵볶음, 콩나물파채무침, 포기김치, 양상...\n",
       "9     [밥, 장각백숙, 적어양념장구이, 채소스틱*쌈장, 도라지오이초무침, 겉절이김치, 양...\n",
       "10    [유니짜장밥, 짬뽕국, 수제찹쌀꿔바로우, 계란후라이, 단무지락교무침, 포기김치, 그...\n",
       "11    [밥, 떡국, 소갈비찜, 한식잡채, 참나물겉절이, 포기김치, 양상추샐러드*블루베리요...\n",
       "12    [밥, 육개장, 닭살겨자냉채, 오이스틱*쌈장, 탕평채, 깍두기/수박, 양상추샐러드*...\n",
       "13    [밥, 미니쌀국수, 삼겹살고추장구이, 스프링롤*타르타르D, 동초나물무침, 알타리김치...\n",
       "14    [밥, 김치어묵탕, 수원왕갈비통닭, 두부양념조림, 연근깨소스무침, 포기김치, 양상추...\n",
       "15    [밥, 유부장국, 해물누룽지탕, 김치전, 마약계란장조림, 포기김치, 양상추샐러드*딸기D]\n",
       "16    [밥, 호박고추장찌개, 안동찜닭, 마카로니치즈범벅, 세발나물무침, 포기김치/요구르트...\n",
       "17         [밥, 근대국, 등갈비김치찜, 감자채전*장, 치커리무침, 깍두기, 파스타샐러드]\n",
       "18    [밥, 해물탕, 쇠고기숙주볶음, 맛살계란말이, 물미역초고추장무침, 포기김치, 양상추...\n",
       "19    [밥, 나주곰탕, 생선까스*타르타르D, 더덕양념구이, 방풍나물무침, 석박지, 그린샐...\n",
       "20      [밥, 옹심이국, 목살스테이크, 베이비크랩강정, 포기김치/부럼, 양상추샐러드*망고D]\n",
       "21    [밥, 아욱국, 치즈불닭, 베이컨감자볶음, 매운콩나물무침, 포기김치, 양배추샐러드*...\n",
       "22    [밥, 황태미역국, 동파육, 느타리버섯볶음, 참나물상추겉절이, 포기김치/망고푸딩, ...\n",
       "23    [밥, 매운쇠고기샤브샤브국, 갈치조림, 수수부꾸미, 쑥갓두부무침, 알타리김치, 양상...\n",
       "24     [밥, 쑥국, 닭다리튀김, 골뱅이채소무침, 미나리나물, 포기김치, 양상추샐러드*참깨D]\n",
       "25        [밥, 얼갈이된장국, 오리불고기, 깻잎무쌈, 포기김치, 양상추샐러드*딸기요거트D]\n",
       "26     [밥, 갈비탕, 순살닭강정, 매생이전, 도라지오이생채, 깍두기, 양상추샐러드*오렌지D]\n",
       "27    [봄나물비빔밥, 냉이된장국, 수제고기육전, 도토리묵*양념장, 쥬시쿨, 포기김치, 콥...\n",
       "28    [밥, 콩가루배추국, 타워함박스테이크, 문어꽈리고추조림, 시금치고추장나물무침, 포기...\n",
       "29    [밥, 어묵매운탕, 목살구이, 쌈채소*쌈장, 부추무침, 겉절이김치, 양상추샐러드*콩...\n",
       "30    [밥, 근대된장국, 묵은지닭찜, 비엔나브로콜리볶음, 유부채소겨자냉채, 깍두기/요구르...\n",
       "31     [밥, 대파육개장, 고등어구이*와사비장, 어묵잡채, 건다래순볶음, 포기김치, 콩샐러드]\n",
       "32    [밥, 물만둣국, 제육미나리볶음, 두부까스*소스, 아삭이고추된장무침, 겉절이김치, ...\n",
       "33    [밥, 열무된장국, 장각허브오븐구이*청양마요소스, 수제오미산적, 머위나물, 포기김치...\n",
       "34           [밥, 버섯매운탕, 돈갈비찜, 오이생채, 포기김치, 그린샐러드*황도요거트D]\n",
       "35    [밥, 황태국, 콩나물불고기, 쇠고기납작당면볶음, 삼색유자청무침, 포기김치, 마카로...\n",
       "36    [밥, 순남시래기국, 장어강정*데리야끼소스, 깻잎쌈*생강채, 유채나물된장무침, 겉절...\n",
       "37    [밥, 미역국, 바베큐폭립, 건새우호박채전, 비름나물, 포기김치, 양상추샐러드*블루...\n",
       "38     [밥, 아욱국, 짜파치킨, 쫄면채소무침, 취나물무침, 포기김치, 양배추샐러드*사우전D]\n",
       "39    [밥, 돈육김치찌개, 소불고기, 가지나물, 풋마늘대무침, 깍두기, 양상추샐러드*흑임자D]\n",
       "40    [밥, 동태매운탕, 차돌박이구이&청경채찜, 메추리알떡볶이, 세발나물무침, 포기김치/...\n",
       "41    [밥, 유부장국, 돈수육, 브로콜리땅콩소스무침, 모듬채소*쌈장, 수제보쌈김치, 양상...\n",
       "42    [밥, 콩가루배추국, 허니순살치킨, 버섯초장무침, 방풍나물, 포기김치, 양상추샐러드...\n",
       "43    [밥, 순대국밥*다대기, 해물동그랑땡채소볶음, 통들깨부추무침, 채소스틱, 석박지/바...\n",
       "44    [밥, 닭개장, 돈육춘장볶음, 김말이강정, 꼬시래기무침, 포기김치, 양상추샐러드*석류D]\n",
       "45    [밥, 쇠고기미역국, 춘천닭갈비, 오지치즈후라이, 가지두반장볶음, 포기김치, 양상추...\n",
       "46    [밥, 순두부백탕, 매콤소갈비찜, 깻잎완자전, 돌나물초장무침, 포기김치, 시리얼샐러...\n",
       "47    [밥, 냉이국, 돈육간장불고기, 비빔냉면, 오이나물볶음, 겉절이김치, 양상추샐러드*...\n",
       "48         [밥, 맑은떡국, 가자미구이*장, 유채나물무침, 포기김치, 양상추샐러드*자몽D]\n",
       "49    [밥, 사골우거지국, 해물누룽지탕, 청포묵*양념간장, 비름나물고추장무침, 석박지, ...\n",
       "Name: lunch_menu, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['lunch_menu'] = test['중식메뉴'].apply(sep_lunch)\n",
    "test['lunch_menu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lunch_bob'] = train['lunch_menu'].apply(lambda x: x[0])\n",
    "train['lunch_soup'] = train['lunch_menu'].apply(lambda x: x[1])\n",
    "train['lunch_main'] = train['lunch_menu'].apply(lambda x: x[2])\n",
    "test['lunch_bob'] = test['lunch_menu'].apply(lambda x: x[0])\n",
    "test['lunch_soup'] = test['lunch_menu'].apply(lambda x: x[1])\n",
    "test['lunch_main'] = test['lunch_menu'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['lunch_menu'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['조식메뉴'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['lunch_dessert'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dinner_menu'] = train['석식메뉴'].apply(sep_lunch)\n",
    "test['dinner_menu'] = test['석식메뉴'].apply(sep_lunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dinner_menu'] = train['석식메뉴'].apply(sep_lunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobd=[]\n",
    "soupd=[]\n",
    "maind=[]\n",
    "\n",
    "for word in  train['dinner_menu']:\n",
    "    if len(word) == 0:\n",
    "        bobd.append('None') #비어있으면 그 날짜자체가 없어질수있으므로 'None' 값으로 대체\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의달' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기계발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*자기계발의날*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기개발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "\n",
    "    else:\n",
    "        bobd.append(word[0])\n",
    "        soupd.append(word[1])\n",
    "        maind.append(word[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobd=[]\n",
    "soupd=[]\n",
    "maind=[]\n",
    "\n",
    "for word in  test['dinner_menu']:\n",
    "    if len(word) == 0:\n",
    "        bobd.append('None') #비어있으면 그 날짜자체가 없어질수있으므로 'None' 값으로 대체\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의달' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기계발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*자기계발의날*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기개발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "\n",
    "    else:\n",
    "        bobd.append(word[0])\n",
    "        soupd.append(word[1])\n",
    "        maind.append(word[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train['dinner_menu']))\n",
    "print(len(bobd))\n",
    "print(len(soupd))\n",
    "print(len(maind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dinner_bob'] = bobd\n",
    "train['dinner_soup'] = soupd\n",
    "train['dinner_main'] = maind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['dinner_bob'] = bobd\n",
    "test['dinner_soup'] = soupd\n",
    "test['dinner_main'] = maind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['dinner)_main'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['중식메뉴', '석식메뉴', 'dinner'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('dinner_menu', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['출근자수'] = train['출근자수'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "sns.barplot(x=\"요일\", y=\"중식계\", data=train)\n",
    "plt.title(\"요일별 점심 식사 수\")\n",
    "plt.show()\n",
    "\n",
    "#월요일이 가장 많고 금요일로 갈수록 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "sns.barplot(x=\"요일\", y=\"석식계\", data=train)\n",
    "plt.title(\"요일별 석식 식사 수\")\n",
    "plt.show()\n",
    "\n",
    "#월요일이 가장 많고 금요일로 갈수록 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#월별 점심떄 사람수\n",
    "plt.figure(figsize=(20,10))\n",
    "rot = sns.boxplot(x='month',y='중식계', data = train)\n",
    "\n",
    "for item in rot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "#2017 년부 12월에 먹는 사람수가 좀 줄어든다.\n",
    "#연말이라 회식이많은것같다. 그래서 밖에서 사먹고 와서 그런것같다.\n",
    "#생각보다 코라나로인해 확 줄거나 그런것은 안보이는 것같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "\n",
    "train['요일'] =  train['요일'].astype('category')\n",
    "train['요일'] = train.요일.cat.codes\n",
    "\n",
    "\n",
    "train['lunch_bob'] =  train['lunch_bob'].astype('category')\n",
    "train['lunch_bob'] = train.lunch_bob.cat.codes\n",
    "\n",
    "train['lunch_soup'] =  train['lunch_soup'].astype('category')\n",
    "train['lunch_soup'] = train.lunch_soup.cat.codes\n",
    "\n",
    "train['lunch_main'] =  train['lunch_main'].astype('category')\n",
    "train['lunch_main'] = train.lunch_main.cat.codes\n",
    "\n",
    "train['dinner_bob'] =  train['dinner_bob'].astype('category')\n",
    "train['dinner_bob'] = train.dinner_bob.cat.codes\n",
    "\n",
    "train['dinner_soup'] =  train['dinner_soup'].astype('category')\n",
    "train['dinner_soup'] = train.dinner_soup.cat.codes\n",
    "\n",
    "train['dinner_main'] =  train['dinner_main'].astype('category')\n",
    "train['dinner_main'] = train.dinner_main.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>요일</th>\n",
       "      <th>본사정원수</th>\n",
       "      <th>본사휴가자수</th>\n",
       "      <th>본사출장자수</th>\n",
       "      <th>본사시간외근무명령서승인건수</th>\n",
       "      <th>현본사소속재택근무자수</th>\n",
       "      <th>조식메뉴</th>\n",
       "      <th>중식메뉴</th>\n",
       "      <th>석식메뉴</th>\n",
       "      <th>...</th>\n",
       "      <th>출근자수</th>\n",
       "      <th>식사자수</th>\n",
       "      <th>lunch_menu</th>\n",
       "      <th>lunch_bob</th>\n",
       "      <th>lunch_soup</th>\n",
       "      <th>lunch_main</th>\n",
       "      <th>dinner_menu</th>\n",
       "      <th>dinner_bob</th>\n",
       "      <th>dinner_soup</th>\n",
       "      <th>dinner_main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>3</td>\n",
       "      <td>2601</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "      <td>238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/찐빵  우유/두유/주스 계란후라이  호두죽/쌀밥 (쌀:국내산) 된장찌개  쥐...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2639.0</td>\n",
       "      <td>[밥, 오징어찌개, 쇠불고기, 계란찜, 청포묵무침, 요구르트, 포기김치]</td>\n",
       "      <td>23</td>\n",
       "      <td>189</td>\n",
       "      <td>236</td>\n",
       "      <td>[밥, 육개장, 자반고등어구이, 두부조림, 건파래무침, 포기김치]</td>\n",
       "      <td>39</td>\n",
       "      <td>224</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-02</td>\n",
       "      <td>4</td>\n",
       "      <td>2601</td>\n",
       "      <td>50</td>\n",
       "      <td>173</td>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/단호박샌드  우유/두유/주스 계란후라이  팥죽/쌀밥 (쌀:국내산) 호박젓국찌...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...</td>\n",
       "      <td>콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...</td>\n",
       "      <td>...</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>2697.0</td>\n",
       "      <td>[밥, 김치찌개, 가자미튀김, 모둠소세지구이, 마늘쫑무침, 요구르트, 배추겉절이]</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>[콩나물밥*양념장, 어묵국, 유산슬, 아삭고추무침, 바나나, 포기김치]</td>\n",
       "      <td>106</td>\n",
       "      <td>183</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>2</td>\n",
       "      <td>2601</td>\n",
       "      <td>56</td>\n",
       "      <td>180</td>\n",
       "      <td>111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/베이글  우유/두유/주스 계란후라이  표고버섯죽/쌀밥 (쌀:국내산) 콩나물국...</td>\n",
       "      <td>카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...</td>\n",
       "      <td>...</td>\n",
       "      <td>2365.0</td>\n",
       "      <td>2476.0</td>\n",
       "      <td>[카레덮밥, 팽이장국, 치킨핑거, 쫄면야채무침, 견과류조림, 요구르트, 포기김치]</td>\n",
       "      <td>50</td>\n",
       "      <td>249</td>\n",
       "      <td>334</td>\n",
       "      <td>[밥, 청국장찌개, 황태양념구이, 고기전, 새송이버섯볶음, 포기김치]</td>\n",
       "      <td>39</td>\n",
       "      <td>244</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-04</td>\n",
       "      <td>1</td>\n",
       "      <td>2601</td>\n",
       "      <td>104</td>\n",
       "      <td>220</td>\n",
       "      <td>355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/토마토샌드  우유/두유/주스 계란후라이  닭죽/쌀밥 (쌀,닭:국내산) 근대국...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...</td>\n",
       "      <td>미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...</td>\n",
       "      <td>...</td>\n",
       "      <td>2277.0</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>[밥, 쇠고기무국, 주꾸미볶음, 부추전, 시금치나물, 요구르트, 포기김치]</td>\n",
       "      <td>23</td>\n",
       "      <td>149</td>\n",
       "      <td>309</td>\n",
       "      <td>[미니김밥*겨자장, 우동, 멕시칸샐러드, 군고구마, 무피클, 포기김치]</td>\n",
       "      <td>35</td>\n",
       "      <td>213</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>2601</td>\n",
       "      <td>278</td>\n",
       "      <td>181</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>모닝롤/와플  우유/두유/주스 계란후라이  쇠고기죽/쌀밥 (쌀:국내산) 재첩국  방...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...</td>\n",
       "      <td>...</td>\n",
       "      <td>2142.0</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>[밥, 떡국, 돈육씨앗강정, 우엉잡채, 청경채무침, 요구르트, 포기김치]</td>\n",
       "      <td>23</td>\n",
       "      <td>75</td>\n",
       "      <td>106</td>\n",
       "      <td>[밥, 차돌박이찌개, 닭갈비, 감자소세지볶음, 콩나물무침, 포기김치]</td>\n",
       "      <td>39</td>\n",
       "      <td>240</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>2</td>\n",
       "      <td>2983</td>\n",
       "      <td>75</td>\n",
       "      <td>198</td>\n",
       "      <td>4</td>\n",
       "      <td>391.0</td>\n",
       "      <td>모닝롤/페퍼로니피자 우유/주스 계란후라이/찐계란 크루통크림스프/흑미밥 아귀지리 마늘...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 아욱국 수제함박스테이크 견과류마카로니범벅 생깻잎지 단호박물김치...</td>\n",
       "      <td>김치볶음밥 미니쫄우동*맛살튀김 브로콜리깨소스무침 계란후라이 고들빼기무침 겉절이김치</td>\n",
       "      <td>...</td>\n",
       "      <td>2319.0</td>\n",
       "      <td>2323.0</td>\n",
       "      <td>[밥, 아욱국, 수제함박스테이크, 견과류마카로니범벅, 생깻잎지, 단호박물김치, 양상...</td>\n",
       "      <td>23</td>\n",
       "      <td>167</td>\n",
       "      <td>248</td>\n",
       "      <td>[김치볶음밥, 미니쫄우동*맛살튀김, 브로콜리깨소스무침, 계란후라이, 고들빼기무침, ...</td>\n",
       "      <td>13</td>\n",
       "      <td>115</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>2983</td>\n",
       "      <td>92</td>\n",
       "      <td>231</td>\n",
       "      <td>462</td>\n",
       "      <td>351.0</td>\n",
       "      <td>모닝롤/생크림단팥빵 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 떡국 해물땡굴소스볶...</td>\n",
       "      <td>쌀밥/수수밥/찰현미밥 냉이된장국 동파육 봄동전 청경채/버섯숙회*초장 무생채 양상추샐...</td>\n",
       "      <td>흑미밥 쇠고기무국 삼치양념구이 비엔나채소볶음 숙주나물당근무침 포기김치</td>\n",
       "      <td>...</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>2771.0</td>\n",
       "      <td>[밥, 냉이된장국, 동파육, 봄동전, 청경채/버섯숙회*초장, 무생채, 양상추샐러드*...</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>115</td>\n",
       "      <td>[흑미밥, 쇠고기무국, 삼치양념구이, 비엔나채소볶음, 숙주나물당근무침, 포기김치]</td>\n",
       "      <td>120</td>\n",
       "      <td>159</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>2983</td>\n",
       "      <td>255</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>303.0</td>\n",
       "      <td>모닝롤/BLT샌드위치 우유/주스 계란후라이/찐계란 흑임자죽/흑미밥 바지락살국 두부조...</td>\n",
       "      <td>전주비빔밥*약고추장 계란파국 요거닭 올방개묵무침 파프리카해초무침 포기김치 양상추샐러...</td>\n",
       "      <td>흑미밥 수제비국 수제맛쵸킹탕수육 유부채소겨자냉채 참나물무침 갓김치/겉절이김치</td>\n",
       "      <td>...</td>\n",
       "      <td>2177.0</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>[전주비빔밥*약고추장, 계란파국, 요거닭, 올방개묵무침, 파프리카해초무침, 포기김치...</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>291</td>\n",
       "      <td>[흑미밥, 수제비국, 수제맛쵸킹탕수육, 유부채소겨자냉채, 참나물무침, 갓김치/겉절이김치]</td>\n",
       "      <td>120</td>\n",
       "      <td>163</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>2983</td>\n",
       "      <td>107</td>\n",
       "      <td>153</td>\n",
       "      <td>616</td>\n",
       "      <td>327.0</td>\n",
       "      <td>모닝롤/호박고구마오븐구이 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 감자양파국 분...</td>\n",
       "      <td>쌀밥/흑미밥/찰현미밥 전주식콩나물해장국 돈육간장불고기 깐풍연근 연두부*달래양념장 봄...</td>\n",
       "      <td>흑미밥 열무된장국 장어강정*데리야끼소스 깻잎쌈*생강채 오이선 포기김치</td>\n",
       "      <td>...</td>\n",
       "      <td>2396.0</td>\n",
       "      <td>3012.0</td>\n",
       "      <td>[밥, 전주식콩나물해장국, 돈육간장불고기, 깐풍연근, 연두부*달래양념장, 봄동겉절이...</td>\n",
       "      <td>23</td>\n",
       "      <td>215</td>\n",
       "      <td>94</td>\n",
       "      <td>[흑미밥, 열무된장국, 장어강정*데리야끼소스, 깻잎쌈*생강채, 오이선, 포기김치]</td>\n",
       "      <td>120</td>\n",
       "      <td>194</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>2021-01-26</td>\n",
       "      <td>4</td>\n",
       "      <td>2983</td>\n",
       "      <td>69</td>\n",
       "      <td>183</td>\n",
       "      <td>551</td>\n",
       "      <td>362.0</td>\n",
       "      <td>모닝롤/야채샌드 우유/주스 계란후라이/찐계란 참치죽/흑미밥 홍합탕 애호박새우젓볶음 ...</td>\n",
       "      <td>쌀밥/귀리밥/찰현미밥 들깨미역국 교촌간장치킨 옥수수콘치즈구이 가지고추장무침 포기김치...</td>\n",
       "      <td>(New)할라피뇨멸치주먹밥 잔치국수 수제고기육전 쑥갓나물 양파초절임 깍두기</td>\n",
       "      <td>...</td>\n",
       "      <td>2369.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>[밥, 들깨미역국, 교촌간장치킨, 옥수수콘치즈구이, 가지고추장무침, 포기김치/요구르...</td>\n",
       "      <td>23</td>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>[잔치국수, 수제고기육전, 쑥갓나물, 양파초절임, 깍두기]</td>\n",
       "      <td>79</td>\n",
       "      <td>162</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1205 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             일자  요일  본사정원수  본사휴가자수  본사출장자수  본사시간외근무명령서승인건수  현본사소속재택근무자수  \\\n",
       "0    2016-02-01   3   2601      50     150             238          0.0   \n",
       "1    2016-02-02   4   2601      50     173             319          0.0   \n",
       "2    2016-02-03   2   2601      56     180             111          0.0   \n",
       "3    2016-02-04   1   2601     104     220             355          0.0   \n",
       "4    2016-02-05   0   2601     278     181              34          0.0   \n",
       "...         ...  ..    ...     ...     ...             ...          ...   \n",
       "1200 2021-01-20   2   2983      75     198               4        391.0   \n",
       "1201 2021-01-21   1   2983      92     231             462        351.0   \n",
       "1202 2021-01-22   0   2983     255     248               1        303.0   \n",
       "1203 2021-01-25   3   2983     107     153             616        327.0   \n",
       "1204 2021-01-26   4   2983      69     183             551        362.0   \n",
       "\n",
       "                                                   조식메뉴  \\\n",
       "0     모닝롤/찐빵  우유/두유/주스 계란후라이  호두죽/쌀밥 (쌀:국내산) 된장찌개  쥐...   \n",
       "1     모닝롤/단호박샌드  우유/두유/주스 계란후라이  팥죽/쌀밥 (쌀:국내산) 호박젓국찌...   \n",
       "2     모닝롤/베이글  우유/두유/주스 계란후라이  표고버섯죽/쌀밥 (쌀:국내산) 콩나물국...   \n",
       "3     모닝롤/토마토샌드  우유/두유/주스 계란후라이  닭죽/쌀밥 (쌀,닭:국내산) 근대국...   \n",
       "4     모닝롤/와플  우유/두유/주스 계란후라이  쇠고기죽/쌀밥 (쌀:국내산) 재첩국  방...   \n",
       "...                                                 ...   \n",
       "1200  모닝롤/페퍼로니피자 우유/주스 계란후라이/찐계란 크루통크림스프/흑미밥 아귀지리 마늘...   \n",
       "1201  모닝롤/생크림단팥빵 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 떡국 해물땡굴소스볶...   \n",
       "1202  모닝롤/BLT샌드위치 우유/주스 계란후라이/찐계란 흑임자죽/흑미밥 바지락살국 두부조...   \n",
       "1203  모닝롤/호박고구마오븐구이 우유/주스 계란후라이/찐계란 누룽지탕/흑미밥 감자양파국 분...   \n",
       "1204  모닝롤/야채샌드 우유/주스 계란후라이/찐계란 참치죽/흑미밥 홍합탕 애호박새우젓볶음 ...   \n",
       "\n",
       "                                                   중식메뉴  \\\n",
       "0     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...   \n",
       "1     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...   \n",
       "2     카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...   \n",
       "3     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...   \n",
       "4     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...   \n",
       "...                                                 ...   \n",
       "1200  쌀밥/흑미밥/찰현미밥 아욱국 수제함박스테이크 견과류마카로니범벅 생깻잎지 단호박물김치...   \n",
       "1201  쌀밥/수수밥/찰현미밥 냉이된장국 동파육 봄동전 청경채/버섯숙회*초장 무생채 양상추샐...   \n",
       "1202  전주비빔밥*약고추장 계란파국 요거닭 올방개묵무침 파프리카해초무침 포기김치 양상추샐러...   \n",
       "1203  쌀밥/흑미밥/찰현미밥 전주식콩나물해장국 돈육간장불고기 깐풍연근 연두부*달래양념장 봄...   \n",
       "1204  쌀밥/귀리밥/찰현미밥 들깨미역국 교촌간장치킨 옥수수콘치즈구이 가지고추장무침 포기김치...   \n",
       "\n",
       "                                                   석식메뉴  ...    출근자수    식사자수  \\\n",
       "0     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...  ...  2401.0  2639.0   \n",
       "1     콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...  ...  2378.0  2697.0   \n",
       "2     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...  ...  2365.0  2476.0   \n",
       "3     미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...  ...  2277.0  2632.0   \n",
       "4     쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...  ...  2142.0  2176.0   \n",
       "...                                                 ...  ...     ...     ...   \n",
       "1200     김치볶음밥 미니쫄우동*맛살튀김 브로콜리깨소스무침 계란후라이 고들빼기무침 겉절이김치   ...  2319.0  2323.0   \n",
       "1201            흑미밥 쇠고기무국 삼치양념구이 비엔나채소볶음 숙주나물당근무침 포기김치   ...  2309.0  2771.0   \n",
       "1202        흑미밥 수제비국 수제맛쵸킹탕수육 유부채소겨자냉채 참나물무침 갓김치/겉절이김치   ...  2177.0  2178.0   \n",
       "1203            흑미밥 열무된장국 장어강정*데리야끼소스 깻잎쌈*생강채 오이선 포기김치   ...  2396.0  3012.0   \n",
       "1204         (New)할라피뇨멸치주먹밥 잔치국수 수제고기육전 쑥갓나물 양파초절임 깍두기   ...  2369.0  2920.0   \n",
       "\n",
       "                                             lunch_menu  lunch_bob  \\\n",
       "0              [밥, 오징어찌개, 쇠불고기, 계란찜, 청포묵무침, 요구르트, 포기김치]         23   \n",
       "1         [밥, 김치찌개, 가자미튀김, 모둠소세지구이, 마늘쫑무침, 요구르트, 배추겉절이]         23   \n",
       "2         [카레덮밥, 팽이장국, 치킨핑거, 쫄면야채무침, 견과류조림, 요구르트, 포기김치]         50   \n",
       "3             [밥, 쇠고기무국, 주꾸미볶음, 부추전, 시금치나물, 요구르트, 포기김치]         23   \n",
       "4              [밥, 떡국, 돈육씨앗강정, 우엉잡채, 청경채무침, 요구르트, 포기김치]         23   \n",
       "...                                                 ...        ...   \n",
       "1200  [밥, 아욱국, 수제함박스테이크, 견과류마카로니범벅, 생깻잎지, 단호박물김치, 양상...         23   \n",
       "1201  [밥, 냉이된장국, 동파육, 봄동전, 청경채/버섯숙회*초장, 무생채, 양상추샐러드*...         23   \n",
       "1202  [전주비빔밥*약고추장, 계란파국, 요거닭, 올방개묵무침, 파프리카해초무침, 포기김치...         46   \n",
       "1203  [밥, 전주식콩나물해장국, 돈육간장불고기, 깐풍연근, 연두부*달래양념장, 봄동겉절이...         23   \n",
       "1204  [밥, 들깨미역국, 교촌간장치킨, 옥수수콘치즈구이, 가지고추장무침, 포기김치/요구르...         23   \n",
       "\n",
       "      lunch_soup  lunch_main  \\\n",
       "0            189         236   \n",
       "1             28           8   \n",
       "2            249         334   \n",
       "3            149         309   \n",
       "4             75         106   \n",
       "...          ...         ...   \n",
       "1200         167         248   \n",
       "1201          41         115   \n",
       "1202          16         291   \n",
       "1203         215          94   \n",
       "1204          69          44   \n",
       "\n",
       "                                            dinner_menu  dinner_bob  \\\n",
       "0                  [밥, 육개장, 자반고등어구이, 두부조림, 건파래무침, 포기김치]          39   \n",
       "1               [콩나물밥*양념장, 어묵국, 유산슬, 아삭고추무침, 바나나, 포기김치]         106   \n",
       "2                [밥, 청국장찌개, 황태양념구이, 고기전, 새송이버섯볶음, 포기김치]          39   \n",
       "3               [미니김밥*겨자장, 우동, 멕시칸샐러드, 군고구마, 무피클, 포기김치]          35   \n",
       "4                [밥, 차돌박이찌개, 닭갈비, 감자소세지볶음, 콩나물무침, 포기김치]          39   \n",
       "...                                                 ...         ...   \n",
       "1200  [김치볶음밥, 미니쫄우동*맛살튀김, 브로콜리깨소스무침, 계란후라이, 고들빼기무침, ...          13   \n",
       "1201      [흑미밥, 쇠고기무국, 삼치양념구이, 비엔나채소볶음, 숙주나물당근무침, 포기김치]         120   \n",
       "1202  [흑미밥, 수제비국, 수제맛쵸킹탕수육, 유부채소겨자냉채, 참나물무침, 갓김치/겉절이김치]         120   \n",
       "1203      [흑미밥, 열무된장국, 장어강정*데리야끼소스, 깻잎쌈*생강채, 오이선, 포기김치]         120   \n",
       "1204                   [잔치국수, 수제고기육전, 쑥갓나물, 양파초절임, 깍두기]          79   \n",
       "\n",
       "     dinner_soup  dinner_main  \n",
       "0            224          341  \n",
       "1            183          335  \n",
       "2            244          430  \n",
       "3            213          172  \n",
       "4            240           92  \n",
       "...          ...          ...  \n",
       "1200         115          213  \n",
       "1201         159          228  \n",
       "1202         163          277  \n",
       "1203         194          345  \n",
       "1204         162          297  \n",
       "\n",
       "[1205 rows x 26 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "\n",
    "test['요일'] =  test['요일'].astype('category')\n",
    "test['요일'] = test.요일.cat.codes\n",
    "\n",
    "\n",
    "test['lunch_bob'] =  test['lunch_bob'].astype('category')\n",
    "test['lunch_bob'] = test.lunch_bob.cat.codes\n",
    "\n",
    "test['lunch_soup'] =  test['lunch_soup'].astype('category')\n",
    "test['lunch_soup'] = test.lunch_soup.cat.codes\n",
    "\n",
    "test['lunch_main'] =  test['lunch_main'].astype('category')\n",
    "test['lunch_main'] = test.lunch_main.cat.codes\n",
    "\n",
    "test['dinner_bob'] =  test['dinner_bob'].astype('category')\n",
    "test['dinner_bob'] = test.dinner_bob.cat.codes\n",
    "\n",
    "test['dinner_soup'] =  test['dinner_soup'].astype('category')\n",
    "test['dinner_soup'] = test.dinner_soup.cat.codes\n",
    "\n",
    "test['dinner_main'] =  test['dinner_main'].astype('category')\n",
    "test['dinner_main'] = test.dinner_main.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lunch_menu'] = test['중식메뉴'].apply(sep_lunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['출근자수'] = test['본사정원수'] - test['본사휴가자수'] - test['본사출장자수'] - test['본사시간외근무명령서승인건수'] - test['현본사소속재택근무자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['year'] = test['일자'].dt.year\n",
    "test['month'] = test['일자'].dt.month\n",
    "test['day']  = test['일자'].dt.day\n",
    "test['dayofweek'] = test['일자'].dt.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
       "       '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴', '중식계', '석식계', 'year', 'month',\n",
       "       'day', 'dayofweek', '출근자수', '식사자수', 'lunch_menu', 'lunch_bob',\n",
       "       'lunch_soup', 'lunch_main', 'dinner_menu', 'dinner_bob', 'dinner_soup',\n",
       "       'dinner_main'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['일자', '요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수',\n",
       "       '현본사소속재택근무자수', '조식메뉴', '중식메뉴', '석식메뉴', 'year', 'month', 'day',\n",
       "       'dayofweek', '출근자수', '식사자수', 'lunch_menu', 'lunch_bob', 'lunch_soup',\n",
       "       'lunch_main', 'dinner_menu', 'dinner_bob', 'dinner_soup',\n",
       "       'dinner_main'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lunch Train 완성\n",
    "#lunch_train\n",
    "train = df[\n",
    "    ['day','numbers','dayoff','work','outsidework','workfhome','lunch_t','Month','Date','bob','soup','main']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[\n",
    "    ['day','numbers','dayoff','work','outsidework','workfhome','dinner_t','Month','Date','bobd','soupd','maind']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_1 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'lunch_bob', 'lunch_soup', 'lunch_main', '출근자수']\n",
    "feature_cols_2 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'dinner_bob', 'dinner_soup', 'dinner_main', '출근자수']\n",
    "feature_cols_3 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'lunch_bob', 'lunch_soup', 'lunch_main', '식사자수']\n",
    "feature_cols_4 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'dinner_bob', 'dinner_soup', 'dinner_main', '식사자수']\n",
    "\n",
    "X_train_1 = train[feature_cols_1]\n",
    "y_train_1 = train['중식계']\n",
    "\n",
    "X_train_2 = train[feature_cols_2]\n",
    "y_train_2 = train['석식계']\n",
    "\n",
    "X_test_1 = test[feature_cols_1]\n",
    "X_test_2 = test[feature_cols_2]\n",
    "\n",
    "X_train_3 = train[feature_cols_3]\n",
    "y_train_3 = train['중식계']\n",
    "\n",
    "X_train_4 = train[feature_cols_4]\n",
    "y_train_4 = train['석식계']\n",
    "\n",
    "X_test_3 = test[feature_cols_3]\n",
    "X_test_4 = test[feature_cols_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_1 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'lunch_bob', 'lunch_soup', 'lunch_main', '출근자수']\n",
    "feature_cols_2 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'dinner_bob', 'dinner_soup', 'dinner_main', '출근자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_3 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'lunch_bob', 'lunch_soup', 'lunch_main', '식사자수']\n",
    "feature_cols_4 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'dinner_bob', 'dinner_soup', 'dinner_main', '식사자수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_1 = ['본사정원수','year', 'month', 'day', 'dayofweek', '출근자수', 'lunch_bob', 'lunch_soup', 'lunch_main']\n",
    "feature_cols_2 = ['본사정원수','year', 'month', 'day', 'dayofweek', '출근자수', 'dinner_bob', 'dinner_soup', 'dinner_main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lunch_bob'] = test['lunch_menu'].apply(lambda x: x[0])\n",
    "test['lunch_soup'] = test['lunch_menu'].apply(lambda x: x[1])\n",
    "test['lunch_main'] = test['lunch_menu'].apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobd=[]\n",
    "soupd=[]\n",
    "maind=[]\n",
    "\n",
    "for word in  test['dinner_menu']:\n",
    "    if len(word) == 0:\n",
    "        bobd.append('None') #비어있으면 그 날짜자체가 없어질수있으므로 'None' 값으로 대체\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '가정의달' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기계발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '*자기계발의날*' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "    elif '자기개발의날' in word:\n",
    "        bobd.append('None')\n",
    "        soupd.append('None')\n",
    "        maind.append('None')\n",
    "\n",
    "    else:\n",
    "        bobd.append(word[0])\n",
    "        soupd.append(word[1])\n",
    "        maind.append(word[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['dinner_menu'] = test['석식메뉴'].apply(sep_lunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['dinner_bob'] = bobd\n",
    "test['dinner_soup'] = soupd\n",
    "test['dinner_main'] = maind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "\n",
    "test['day'] =  test['day'].astype('category')\n",
    "test['day'] = test.day.cat.codes\n",
    "\n",
    "\n",
    "test['lunch_bob'] =  test['lunch_bob'].astype('category')\n",
    "test['lunch_bob'] = test.lunch_bob.cat.codes\n",
    "\n",
    "test['lunch_soup'] =  test['lunch_soup'].astype('category')\n",
    "test['lunch_soup'] = test.lunch_soup.cat.codes\n",
    "\n",
    "test['lunch_main'] =  test['lunch_main'].astype('category')\n",
    "test['lunch_main'] = test.lunch_main.cat.codes\n",
    "\n",
    "test['dinner_bob'] =  test['dinner_bob'].astype('category')\n",
    "test['dinner_bob'] = test.dinner_bob.cat.codes\n",
    "\n",
    "test['dinner_soup'] =  test['dinner_soup'].astype('category')\n",
    "test['dinner_soup'] = test.dinner_soup.cat.codes\n",
    "\n",
    "test['dinner_main'] =  test['dinner_main'].astype('category')\n",
    "test['dinner_main'] = test.dinner_main.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = train[feature_cols_1]\n",
    "y_train_1 = train['중식계']\n",
    "\n",
    "X_train_2 = train[feature_cols_2]\n",
    "y_train_2 = train['석식계']\n",
    "\n",
    "X_test_1 = test[feature_cols_1]\n",
    "X_test_2 = test[feature_cols_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = train[feature_cols_3]\n",
    "y_train_3 = train['중식계']\n",
    "\n",
    "X_train_4 = train[feature_cols_4]\n",
    "y_train_4 = train['석식계']\n",
    "\n",
    "X_test_3 = test[feature_cols_3]\n",
    "X_test_4 = test[feature_cols_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.0.2.\n"
     ]
    }
   ],
   "source": [
    "# 모델링\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_reg_1 = RandomForestRegressor(n_jobs=-1, n_estimators=500, criterion='absolute_error')\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "# Deprecated since version 1.0: Criterion “mae” was deprecated in v1.0 and will be removed in version 1.2. \n",
    "# Use criterion=\"absolute_error\" which is equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_max_features = [0.1, 0.5, 0.9]\n",
    "list_max_depth = [1, 3, 5]\n",
    "n_estimators = 300\n",
    "\n",
    "list_hparam = []\n",
    "\n",
    "for max_depth in list_max_depth:\n",
    "    for max_features in list_max_features:\n",
    "        rf = RandomForestRegressor(n_jobs=-1,criterion='absolute_error', \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_depth = max_depth, \n",
    "                                   max_features = max_features)\n",
    "        \n",
    "        score = cross_val_score(rf, X_train_1, y_train_1, cv=5).mean()\n",
    "        \n",
    "        result = {'score': score, 'n_esti':n_estimators, 'max_depth':max_depth,\n",
    "                 'max_feat':max_features}\n",
    "        \n",
    "        list_hparam.append(result)\n",
    "        # print(max_depth, max_features, score, n_estimators, max_depth, max_features)\n",
    "        print(f\"score: {score} n_esti:{n_estimators} max_depth:{max_depth} max_feat:{max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째\n",
      "score: 0.13168560207842947\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.1\n",
      "\n",
      "2번째\n",
      "score: 0.35406501870517426\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.5\n",
      "\n",
      "3번째\n",
      "score: 0.3637784125144792\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.9\n",
      "\n",
      "4번째\n",
      "score: 0.3373255124043546\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.1\n",
      "\n",
      "5번째\n",
      "score: 0.6309967469670111\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.5\n",
      "\n",
      "6번째\n",
      "score: 0.6440645858561723\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.9\n",
      "\n",
      "7번째\n",
      "score: 0.47187267284817536\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.1\n",
      "\n",
      "8번째\n",
      "score: 0.7126060364938961\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.5\n",
      "\n",
      "9번째\n",
      "score: 0.7244679938668723\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_max_features = [0.1, 0.5, 0.9]\n",
    "list_max_depth = [1, 3, 5]\n",
    "n_estimators = 500\n",
    "\n",
    "list_hparam = []\n",
    "\n",
    "num = 1\n",
    "for max_depth in list_max_depth:\n",
    "    for max_features in list_max_features:\n",
    "        rf = RandomForestRegressor(n_jobs=-1,criterion='absolute_error', \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_depth = max_depth, \n",
    "                                   max_features = max_features)\n",
    "        \n",
    "        score = cross_val_score(rf, X_train_1, y_train_1, cv=5).mean()\n",
    "        \n",
    "        result = {'score': score, 'n_esti':n_estimators, 'max_depth':max_depth,\n",
    "                 'max_feat':max_features}\n",
    "        \n",
    "        list_hparam.append(result)\n",
    "        # print(max_depth, max_features, score, n_estimators, max_depth, max_features)\n",
    "        print(f\"{num}번째\\nscore: {score}\\nn_esti:{n_estimators}\\nmax_depth:{max_depth}\\nmax_feat:{max_features}\")\n",
    "        print()\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째\n",
      "score: 0.1368684038263697\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.1\n",
      "\n",
      "2번째\n",
      "score: 0.35220537717568046\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.5\n",
      "\n",
      "3번째\n",
      "score: 0.3625436079303276\n",
      "n_esti:500\n",
      "max_depth:1\n",
      "max_feat:0.9\n",
      "\n",
      "4번째\n",
      "score: 0.3661264268832237\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.1\n",
      "\n",
      "5번째\n",
      "score: 0.6302136339430883\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.5\n",
      "\n",
      "6번째\n",
      "score: 0.6407876609188113\n",
      "n_esti:500\n",
      "max_depth:3\n",
      "max_feat:0.9\n",
      "\n",
      "7번째\n",
      "score: 0.49245959135663603\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.1\n",
      "\n",
      "8번째\n",
      "score: 0.712045350252621\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.5\n",
      "\n",
      "9번째\n",
      "score: 0.719664499984305\n",
      "n_esti:500\n",
      "max_depth:5\n",
      "max_feat:0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_max_features = [0.1, 0.5, 0.9]\n",
    "list_max_depth = [1, 3, 5]\n",
    "n_estimators = 500\n",
    "\n",
    "list_hparam = []\n",
    "\n",
    "num = 1\n",
    "for max_depth in list_max_depth:\n",
    "    for max_features in list_max_features:\n",
    "        rf = RandomForestRegressor(n_jobs=-1,criterion='absolute_error', \n",
    "                                   n_estimators = n_estimators, \n",
    "                                   max_depth = max_depth, \n",
    "                                   max_features = max_features)\n",
    "        \n",
    "        score = cross_val_score(rf, X_train_3, y_train_3, cv=5).mean()\n",
    "        \n",
    "        result = {'score': score, 'n_esti':n_estimators, 'max_depth':max_depth,\n",
    "                 'max_feat':max_features}\n",
    "        \n",
    "        list_hparam.append(result)\n",
    "        # print(max_depth, max_features, score, n_estimators, max_depth, max_features)\n",
    "        print(f\"{num}번째\\nscore: {score}\\nn_esti:{n_estimators}\\nmax_depth:{max_depth}\\nmax_feat:{max_features}\")\n",
    "        print()\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestRegressor(criterion='absolute_error',\n",
       "                                             n_jobs=-1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       "                         'max_features': [0.1, 0.5, 0.9],\n",
       "                         'n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,20,30,40,50,60,70,80,90],\n",
    "    'max_features' : [0.1, 0.5, 0.9]    \n",
    "}\n",
    "\n",
    "rf_1 = RandomForestRegressor(n_jobs=-1, criterion='absolute_error')\n",
    "\n",
    "grid_cv = GridSearchCV(rf_1, param_grid = params, cv=5, n_jobs=-1)\n",
    "\n",
    "grid_cv.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m rf_1 \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabsolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m grid_cv_test \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_1, param_grid \u001b[38;5;241m=\u001b[39m params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mgrid_cv_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_3\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,20,30,40,50,60,70,80,90],\n",
    "    'max_features' : [0.1, 0.5, 0.9]    \n",
    "}\n",
    "\n",
    "rf_1 = RandomForestRegressor(n_jobs=-1, criterion='absolute_error')\n",
    "\n",
    "grid_cv_test = GridSearchCV(rf_1, param_grid = params, cv=5, n_jobs=-1)\n",
    "\n",
    "grid_cv_test.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 70, 'max_features': 0.5, 'n_estimators': 200}\n",
      "0.7475282220506749\n"
     ]
    }
   ],
   "source": [
    "print(grid_cv.best_params_)\n",
    "print(grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestRegressor(criterion='absolute_error',\n",
       "                                             n_jobs=-1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
       "                         'max_features': [0.1, 0.5, 0.9],\n",
       "                         'n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid_cv_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_cv_test\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(grid_cv_test.best_params_)\n",
    "print(grid_cv_test.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score, n_estimators, max_depth, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>n_esti</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136868</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.352205</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.362544</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.366126</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.492460</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630214</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.640788</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.712045</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.719664</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  n_esti  max_depth  max_feat\n",
       "0  0.136868     500          1       0.1\n",
       "1  0.352205     500          1       0.5\n",
       "2  0.362544     500          1       0.9\n",
       "3  0.366126     500          3       0.1\n",
       "6  0.492460     500          5       0.1\n",
       "4  0.630214     500          3       0.5\n",
       "5  0.640788     500          3       0.9\n",
       "7  0.712045     500          5       0.5\n",
       "8  0.719664     500          5       0.9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list_hparam).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_1 = RandomForestRegressor(n_jobs=-1, n_estimators=10, criterion='absolute_error',\n",
    "                                max_depth=1, max_features=0.1)\n",
    "rf_reg_2 = RandomForestRegressor(n_jobs=-1, n_estimators=10, criterion='absolute_error',\n",
    "                                max_depth=1, max_features=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_1.fit(X_train_1, y_train_1)\n",
    "\n",
    "ftr_importance = pd.Series(rf_reg_1.feature_importances_, \n",
    "                           index = X_train_1.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=ftr_importance, y=ftr_importance.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "pred_1 = rf_reg_1.predict(X_test_1)\n",
    "pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2 = rf_reg_2.predict(X_test_2)\n",
    "pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['중식계'] = pred_1\n",
    "submission['석식계'] = pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data_dacon/dacon_submission-my1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "num_epoch = 10\n",
    "hyper_list_1 = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=2, high=100)\n",
    "    max_features = np.random.uniform(low=0.1, high=1.0)\n",
    "    \n",
    "    rf = RandomForestRegressor(n_jobs=-1, criterion='squared_error', \n",
    "                               n_estimators = n_estimators, \n",
    "                               max_depth = max_depth, \n",
    "                               max_features = max_features)\n",
    "    \n",
    "    score = cross_val_score(rf, X_train_1, y_train_1, cv=5).mean()\n",
    "    \n",
    "    h_params = {\n",
    "        'epoch' : epoch,\n",
    "        'score' : score,\n",
    "        'n_estimators' : n_estimators,\n",
    "        'max_depth' : max_depth,\n",
    "        'max_features' : max_features\n",
    "    }  \n",
    "    \n",
    "    hyper_list_1.append(h_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(hyper_list_1).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "num_epoch = 10\n",
    "hyper_list_2 = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    max_depth = np.random.randint(low=2, high=100)\n",
    "    max_features = np.random.uniform(low=0.1, high=1.0)\n",
    "    \n",
    "    rf = RandomForestRegressor(n_jobs=-1, criterion='absolute_error', \n",
    "                               n_estimators = n_estimators, \n",
    "                               max_depth = max_depth, \n",
    "                               max_features = max_features)\n",
    "    \n",
    "    score = cross_val_score(rf, X_train_2, y_train_2, cv=5).mean()\n",
    "    \n",
    "    h_params = {\n",
    "        'epoch' : epoch,\n",
    "        'score' : score,\n",
    "        'n_estimators' : n_estimators,\n",
    "        'max_depth' : max_depth,\n",
    "        'max_features' : max_features\n",
    "    }  \n",
    "    \n",
    "    hyper_list_2.append(h_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(hyper_list_2).sort_values(by='score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_1 = RandomForestRegressor(n_jobs=-1, n_estimators=300, criterion='absolute_error',\n",
    "                                max_depth=3, max_features=0.407424)\n",
    "rf_reg_2 = RandomForestRegressor(n_jobs=-1, n_estimators=300, criterion='absolute_error',\n",
    "                                max_depth=3, max_features=0.407424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_1.fit(X_train_1, y_train_1)\n",
    "\n",
    "ftr_importance = pd.Series(rf_reg_1.feature_importances_, \n",
    "                           index = X_train_1.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=ftr_importance, y=ftr_importance.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "pred_1 = rf_reg_1.predict(X_test_1)\n",
    "pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2 = rf_reg_2.predict(X_test_2)\n",
    "pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['중식계'] = pred_1\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['석식계'] = pred_2\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data_dacon/dacon_submission-my2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "lgbm_1 = lgbm.LGBMRegressor(learning_rate=0.1, n_estimators=500)\n",
    "lgbm_2 = lgbm.LGBMRegressor(learning_rate=0.1, n_estimators=500)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1476\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 886.710581\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1477\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 886.352697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1482\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 891.338174\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1470\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 888.247925\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 899.022822\n"
     ]
    }
   ],
   "source": [
    "lgbms_1 = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_1):\n",
    "    X_1 = X_train_1.iloc[train_idx]\n",
    "    y_1 = y_train_1.iloc[train_idx]\n",
    "    X_1_val = X_train_1.iloc[val_idx]\n",
    "    y_1_val = y_train_1.iloc[val_idx]\n",
    "    \n",
    "    lgbms_1.append(lgbm_1.fit(X_1, y_1, eval_set=(X_1_val, y_1_val) )) #, early_stopping_rounds=100, verbose=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1601\n",
      "[LightGBM] [Info] Number of data points in the train set: 1205, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 890.334440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1641\n",
      "[LightGBM] [Info] Number of data points in the train set: 1205, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 461.772614\n"
     ]
    }
   ],
   "source": [
    "lgbm_1.fit(X_train_1, y_train_1)\n",
    "pred_1 = lgbm_1.predict(X_test_1)\n",
    "\n",
    "lgbm_2.fit(X_train_2, y_train_2)\n",
    "pred_2 = lgbm_2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>995.068200</td>\n",
       "      <td>353.104487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>893.186239</td>\n",
       "      <td>420.447777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>528.724547</td>\n",
       "      <td>231.864798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1216.637279</td>\n",
       "      <td>568.688154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>950.501678</td>\n",
       "      <td>412.618028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자          중식계         석식계\n",
       "0  2021-01-27   995.068200  353.104487\n",
       "1  2021-01-28   893.186239  420.447777\n",
       "2  2021-01-29   528.724547  231.864798\n",
       "3  2021-02-01  1216.637279  568.688154\n",
       "4  2021-02-02   950.501678  412.618028"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['중식계'] = pred_1\n",
    "submission['석식계'] = pred_2\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data_dacon/dacon_submission-my3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1476\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 886.710581\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1477\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 886.352697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1482\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 891.338174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1470\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 888.247925\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 899.022822\n"
     ]
    }
   ],
   "source": [
    "lgbms_2 = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_3):\n",
    "    X_3 = X_train_3.iloc[train_idx]\n",
    "    y_3 = y_train_3.iloc[train_idx]\n",
    "    X_3_val = X_train_3.iloc[val_idx]\n",
    "    y_3_val = y_train_3.iloc[val_idx]\n",
    "    \n",
    "    lgbms_2.append(lgbm_2.fit(X_3, y_3, eval_set=(X_3_val, y_3_val) )) #, early_stopping_rounds=100, verbose=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1601\n",
      "[LightGBM] [Info] Number of data points in the train set: 1205, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 890.334440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1641\n",
      "[LightGBM] [Info] Number of data points in the train set: 1205, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 461.772614\n"
     ]
    }
   ],
   "source": [
    "lgbm_2.fit(X_train_3, y_train_3)\n",
    "pred_3 = lgbm_2.predict(X_test_3)\n",
    "\n",
    "lgbm_2.fit(X_train_4, y_train_4)\n",
    "pred_4 = lgbm_2.predict(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>995.542690</td>\n",
       "      <td>360.071418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>952.453163</td>\n",
       "      <td>409.487559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>489.906745</td>\n",
       "      <td>205.867632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1225.213404</td>\n",
       "      <td>558.510364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>962.503175</td>\n",
       "      <td>432.321960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자          중식계         석식계\n",
       "0  2021-01-27   995.542690  360.071418\n",
       "1  2021-01-28   952.453163  409.487559\n",
       "2  2021-01-29   489.906745  205.867632\n",
       "3  2021-02-01  1225.213404  558.510364\n",
       "4  2021-02-02   962.503175  432.321960"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['중식계'] = pred_3\n",
    "submission['석식계'] = pred_4\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data_dacon/dacon_submission-my4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/shinminseog/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.22.4)\n",
      "Requirement already satisfied: scipy in /Users/shinminseog/opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.7.3)\n",
      "Downloading xgboost-2.1.0-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_1, y_train_1)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = grid_search.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.7, 'colsample_bytree': 1, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_2, y_train_2)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = grid_search.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>1090.097290</td>\n",
       "      <td>415.644623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>1025.375610</td>\n",
       "      <td>475.955597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>744.584900</td>\n",
       "      <td>276.476196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1370.823608</td>\n",
       "      <td>591.086914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>1007.453674</td>\n",
       "      <td>508.029846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자          중식계         석식계\n",
       "0  2021-01-27  1090.097290  415.644623\n",
       "1  2021-01-28  1025.375610  475.955597\n",
       "2  2021-01-29   744.584900  276.476196\n",
       "3  2021-02-01  1370.823608  591.086914\n",
       "4  2021-02-02  1007.453674  508.029846"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['중식계'] = y_pred_1\n",
    "submission['석식계'] = y_pred_2\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"data_dacon/dacon_submission-gpu1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.7, 'colsample_bytree': 0.5, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_3, y_train_3)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3 = grid_search.predict(X_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_4, y_train_4)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_4 = grid_search.predict(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['중식계'] = y_pred_3\n",
    "submission['석식계'] = y_pred_4\n",
    "submission.to_csv(\"data_dacon/dacon_submission-gpu2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>중식계</th>\n",
       "      <th>석식계</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>1111.653320</td>\n",
       "      <td>382.472321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>1013.794434</td>\n",
       "      <td>408.206757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>683.064575</td>\n",
       "      <td>257.598511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1333.469727</td>\n",
       "      <td>584.386475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>982.564148</td>\n",
       "      <td>539.794006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           일자          중식계         석식계\n",
       "0  2021-01-27  1111.653320  382.472321\n",
       "1  2021-01-28  1013.794434  408.206757\n",
       "2  2021-01-29   683.064575  257.598511\n",
       "3  2021-02-01  1333.469727  584.386475\n",
       "4  2021-02-02   982.564148  539.794006"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[['day', 'numbers', 'dayoff', 'work', 'outsidework', 'workfhome','Month','Date','bob','soup','main']]\n",
    "y_train = train['lunch_t'] \n",
    "x_test = test[['day', 'numbers', 'dayoff', 'work', 'outsidework', 'workfhome','Month','Date','bob','soup','main']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_test_1 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'lunch_bob', 'lunch_soup', 'lunch_main']\n",
    "feature_cols_test_2 = ['요일', '본사정원수', '본사휴가자수', '본사출장자수', '본사시간외근무명령서승인건수', '현본사소속재택근무자수', 'year', 'month', 'day', 'dinner_bob', 'dinner_soup', 'dinner_main']\n",
    "\n",
    "X_train_test_1 = train[feature_cols_test_1]\n",
    "y_train_test_1 = train['중식계']\n",
    "\n",
    "X_train_test_2 = train[feature_cols_test_2]\n",
    "y_train_test_2 = train['석식계']\n",
    "\n",
    "X_test_test_1 = test[feature_cols_test_1]\n",
    "X_test_test_2 = test[feature_cols_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 1, 'colsample_bytree': 1, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_test_1, y_train_test_1)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred1 = grid_search.predict(X_test_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_test_2, y_train_test_2)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred2 = grid_search.predict(X_test_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['중식계'] = y_test_pred1\n",
    "submission['석식계'] = y_test_pred2\n",
    "submission.to_csv(\"data_dacon/dacon_submission-gpu-menu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 1, 'colsample_bytree': 1, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_absolute_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_1, y_train_1)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = grid_search.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=0, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=0, max_depth=3,\n",
       "             max_leaves=None, min_child_weight=1, missing=None,\n",
       "             monotone_constraints=None, multi_strategy=None, n_estimators=600,\n",
       "             n_jobs=1, nthread=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'n_estimators':range(300,600,100), #  'n_estimators':range(600,700,50) 여기에 cv 10 (이거와 별반차이가 없다.)\n",
    "    'colsample_bytree':[0.5,0.7,1],\n",
    "    'colsample_bylevel':[0.5,0.7,1],\n",
    "}\n",
    "model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param, cv=10, \n",
    "                           scoring='neg_mean_absolute_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_2, y_train_2)\n",
    "print(grid_search.best_params_)\n",
    "{'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'max_depth': 3, 'n_estimators': 600}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
    "             colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=600,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = grid_search.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['중식계'] = y_pred1\n",
    "submission['석식계'] = y_pred2\n",
    "submission.to_csv(\"data_dacon/dacon_submission-gpu-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
