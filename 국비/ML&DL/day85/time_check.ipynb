{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT (시간 체크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# MPS(Apple Silicon GPU) 사용 여부 확인\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            # MPS 사용\n",
    "            batch_embeddings = self.model.encode(batch, convert_to_tensor=True, device=device)\n",
    "            embeddings.extend(batch_embeddings.cpu().tolist())  # GPU에서 CPU로 변환 후 리스트로 저장\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"법제처\\s+.*?\\s+국가법령정보센터\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>|\\[.*?]|\\(.*?\\)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\(.*?\\)\\s*\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "    text = re.sub(r\"\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def tokenizer_len(text: str) -> int:\n",
    "    return len(AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\").encode(text))\n",
    "\n",
    "def process_document(doc: Document) -> List[str]:\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=tokenizer_len,\n",
    "    )\n",
    "    \n",
    "    return splitter.split_text(cleaned_text)\n",
    "\n",
    "def process_file(file_path: str, model_name: str, persist_directory: str):\n",
    "    embedding_model = SentenceTransformerEmbeddings(model_name)\n",
    "    \n",
    "    # 문서 로드 시간 측정\n",
    "    loader_start_time = time.time()\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    loader_end_time = time.time()\n",
    "    print(f\"문서 로드 시간: {loader_end_time - loader_start_time:.2f} 초\")\n",
    "    \n",
    "    # 문서 분할 시간 측정\n",
    "    split_start_time = time.time()\n",
    "    docs_split = [process_document(doc) for doc.page_content in documents]\n",
    "    docs_split = [item for sublist in docs_split for item in sublist]\n",
    "    split_end_time = time.time()\n",
    "    print(f\"문서 분할 시간: {split_end_time - split_start_time:.2f} 초\")\n",
    "\n",
    "    # Chroma 초기화\n",
    "    client = Chroma(embedding_function=embedding_model, persist_directory=persist_directory, anonymized_telemetry=False)\n",
    "\n",
    "    # 벡터 추가 시간 측정\n",
    "    embedding_start_time = time.time()\n",
    "    batch_size = 1000  # Chroma에 추가할 배치 크기\n",
    "    for i in range(0, len(docs_split), batch_size):\n",
    "        batch = docs_split[i:i+batch_size]\n",
    "        client.add_documents(batch)\n",
    "        client.persist()  # 매 배치 처리 후 persist 호출\n",
    "    embedding_end_time = time.time()\n",
    "    print(f\"벡터 추가 및 저장 시간: {embedding_end_time - embedding_start_time:.2f} 초\")\n",
    "\n",
    "    # 메모리 정리\n",
    "    del embedding_model, loader, documents, docs_split, client\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    total_time = embedding_end_time - loader_start_time\n",
    "    return total_time\n",
    "\n",
    "def main():\n",
    "    directory_path = 'file/현행법령/'\n",
    "    persist_directory = \"file/chroma_storage/multilingual_law/\"\n",
    "    model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "    file_names = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"{file_name} 파일 처리 중...\")\n",
    "\n",
    "        total_processing_time = process_file(file_path, model_name, persist_directory)\n",
    "\n",
    "        print(f\"{file_name} 처리 완료. 총 소요 시간: {total_processing_time:.2f} 초\")\n",
    "\n",
    "    print(\"모든 파일 처리 완료.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# MPS(Apple Silicon GPU) 사용 여부 확인\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        batch_embeddings = self.model.encode(texts, convert_to_tensor=True, device=device)\n",
    "        return batch_embeddings.cpu().tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "def clean_text(documents: List[Document]) -> List[Document]:\n",
    "    cleaned_documents = []\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "        text = re.sub(r\"법제처\\s+.*?\\s+국가법령정보센터\", \"\", text)\n",
    "        text = re.sub(r\"<.*?>|\\[.*?]|\\(.*?\\)\", \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(r\"\\(.*?\\)\\s*\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "        text = re.sub(r\"\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        cleaned_documents.append(Document(\n",
    "            page_content=text.strip(),\n",
    "            metadata=doc.metadata\n",
    "        ))\n",
    "    return cleaned_documents\n",
    "\n",
    "def tokenizer_len(text: str) -> int:\n",
    "    # 토크나이저 성능 테스트용 시간 측정\n",
    "    start_time = time.time()\n",
    "    token_length = len(AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\").encode(text))\n",
    "    end_time = time.time()\n",
    "    print(f\"토큰 길이 계산 시간: {end_time - start_time:.2f} 초\")\n",
    "    return token_length\n",
    "\n",
    "def process_document(doc: Document, embedding_model) -> List[str]:\n",
    "    cleaned_text = clean_text(doc)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=lambda text: len(embedding_model.tokenizer.encode(text)),\n",
    "    )\n",
    "\n",
    "    split_start_time = time.time()\n",
    "    split_texts = splitter.split_text(cleaned_text)\n",
    "    split_end_time = time.time()\n",
    "\n",
    "    # 분할된 텍스트의 길이 출력\n",
    "    print(f\"문서 분할 완료: {len(split_texts)}개의 청크 생성, 분할 시간: {split_end_time - split_start_time:.2f} 초\")\n",
    "    return split_texts\n",
    "\n",
    "def process_file(file_path: str, model_name: str, persist_directory: str):\n",
    "    embedding_model = SentenceTransformerEmbeddings(model_name)\n",
    "    \n",
    "    loader_start_time = time.time()\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    loader_end_time = time.time()\n",
    "    print(f\"문서 로드 시간: {loader_end_time - loader_start_time:.2f} 초\")\n",
    "    \n",
    "    docs_split = []\n",
    "    \n",
    "    # 문서 분할 시간 측정 및 각 문서별 처리 시간 확인\n",
    "    for doc in documents:\n",
    "        doc_split_start_time = time.time()\n",
    "        split_texts = process_document(doc, embedding_model)\n",
    "        docs_split.extend(split_texts)\n",
    "        doc_split_end_time = time.time()\n",
    "        print(f\"문서 {idx + 1} 분할 완료, 시간: {doc_split_end_time - doc_split_start_time:.2f} 초\")\n",
    "\n",
    "    # 전체 분할 완료 후 벡터 추가\n",
    "    split_end_time = time.time()\n",
    "    print(f\"전체 문서 분할 완료. 총 생성된 청크 수: {len(docs_split)}, 총 분할 시간: {split_end_time - loader_end_time:.2f} 초\")\n",
    "\n",
    "    client = Chroma(embedding_function=embedding_model, persist_directory=persist_directory)\n",
    "\n",
    "    embedding_start_time = time.time()\n",
    "    client.add_documents(docs_split)\n",
    "    client.persist()\n",
    "    embedding_end_time = time.time()\n",
    "    print(f\"벡터 추가 및 저장 시간: {embedding_end_time - embedding_start_time:.2f} 초\")\n",
    "\n",
    "    del embedding_model, loader, documents, docs_split, client\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    total_time = embedding_end_time - loader_start_time\n",
    "    return total_time\n",
    "\n",
    "def main():\n",
    "    directory_path = 'file/현행법령/'\n",
    "    persist_directory = \"file/chroma_storage/multilingual_law/\"\n",
    "    model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "    file_names = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"{file_name} 파일 처리 중...\")\n",
    "\n",
    "        total_processing_time = process_file(file_path, model_name, persist_directory)\n",
    "\n",
    "        print(f\"{file_name} 처리 완료. 총 소요 시간: {total_processing_time:.2f} 초\")\n",
    "\n",
    "    print(\"모든 파일 처리 완료.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# MPS(Apple Silicon GPU) 사용 여부 확인\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            # MPS 사용\n",
    "            batch_embeddings = self.model.encode(batch, convert_to_tensor=True, device=device)\n",
    "            embeddings.extend(batch_embeddings.cpu().tolist())  # GPU에서 CPU로 변환 후 리스트로 저장\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"법제처\\s+.*?\\s+국가법령정보센터\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>|\\[.*?]|\\(.*?\\)\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\(.*?\\)\\s*\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "    text = re.sub(r\"\\d{2,4}[-.\\s]\\d{3,4}[-.\\s]\\d{4}\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def tokenizer_len(text: str) -> int:\n",
    "    return len(AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\").encode(text))\n",
    "\n",
    "def process_document_parallel(doc: Document) -> List[str]:\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=tokenizer_len,\n",
    "    )\n",
    "    return splitter.split_text(cleaned_text)\n",
    "\n",
    "def process_file(file_path: str, model_name: str, persist_directory: str):\n",
    "    embedding_model = SentenceTransformerEmbeddings(model_name)\n",
    "    \n",
    "    # 문서 로드 시간 측정\n",
    "    loader_start_time = time.time()\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    loader_end_time = time.time()\n",
    "    print(f\"문서 로드 시간: {loader_end_time - loader_start_time:.2f} 초\")\n",
    "\n",
    "    # 병렬로 문서 분할\n",
    "    split_start_time = time.time()\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        docs_split = list(executor.map(process_document_parallel, documents))\n",
    "    \n",
    "    docs_split = [item for sublist in docs_split for item in sublist]\n",
    "    split_end_time = time.time()\n",
    "    print(f\"문서 분할 시간: {split_end_time - split_start_time:.2f} 초\")\n",
    "\n",
    "    # Chroma 초기화\n",
    "    client = Chroma(embedding_function=embedding_model, persist_directory=persist_directory, anonymized_telemetry=False)\n",
    "\n",
    "    # 벡터 추가 시간 측정\n",
    "    embedding_start_time = time.time()\n",
    "    batch_size = 1000  # Chroma에 추가할 배치 크기\n",
    "    for i in range(0, len(docs_split), batch_size):\n",
    "        batch = docs_split[i:i+batch_size]\n",
    "        client.add_documents(batch)\n",
    "        client.persist()  # 매 배치 처리 후 persist 호출\n",
    "    embedding_end_time = time.time()\n",
    "    print(f\"벡터 추가 및 저장 시간: {embedding_end_time - embedding_start_time:.2f} 초\")\n",
    "\n",
    "    # 메모리 정리\n",
    "    del embedding_model, loader, documents, docs_split, client\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    total_time = embedding_end_time - loader_start_time\n",
    "    return total_time\n",
    "\n",
    "def main():\n",
    "    directory_path = 'file/현행법령/'\n",
    "    persist_directory = \"file/chroma_storage/multilingual_law/\"\n",
    "    model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "    file_names = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"{file_name} 파일 처리 중...\")\n",
    "\n",
    "        total_processing_time = process_file(file_path, model_name, persist_directory)\n",
    "\n",
    "        print(f\"{file_name} 처리 완료. 총 소요 시간: {total_processing_time:.2f} 초\")\n",
    "\n",
    "    print(\"모든 파일 처리 완료.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼플렉시티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2w/p0zgpvps62gb8jlzgy_v7zgh0000gn/T/ipykernel_2069/645148011.py:43: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  client.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첨단재생의료 및 첨단바이오의약품 안전 및 지원에 관한 법률(법률)(제20331호)(20240521).pdf 처리 완료.\t소요 시간 : 24.15 초\n",
      "차세대전자소송 추진단 설치 및 운영에 관한 규칙(대법원규칙)(제02874호)(20200101).pdf 처리 완료.\t소요 시간 : 12.24 초\n",
      "민사조정법(법률)(제16910호)(20200305).pdf 처리 완료.\t소요 시간 : 12.47 초\n",
      "취업 후 학자금 상환 특별법 시행규칙(교육부령)(제00333호)(20240701).pdf 처리 완료.\t소요 시간 : 12.19 초\n",
      "국군방첩사령부령(대통령령)(제33409호)(20230418).pdf 처리 완료.\t소요 시간 : 11.06 초\n",
      "정부대표 및 특별사절의 임명과 권한에 관한 법률(법률)(제17160호)(20200331).pdf 처리 완료.\t소요 시간 : 10.24 초\n",
      "달빛철도 건설을 위한 특별법(법률)(제20293호)(20240814).pdf 처리 완료.\t소요 시간 : 12.48 초\n",
      "한국마사회법 시행령(대통령령)(제34577호)(20240621).pdf 처리 완료.\t소요 시간 : 10.77 초\n",
      "연안관리법 시행규칙(해양수산부령)(제00460호)(20210219).pdf 처리 완료.\t소요 시간 : 12.92 초\n",
      "허베이 스피리트호 유류오염사고 피해주민의 지원 및 해양환경의 복원 등에 관한 특별법 시행령(대통령령)(제30977호)(20200828).pdf 처리 완료.\t소요 시간 : 12.95 초\n",
      "실내공기질 관리법(법률)(제19720호)(20240315).pdf 처리 완료.\t소요 시간 : 14.30 초\n",
      "의료ㆍ요양 등 지역 돌봄의 통합지원에 관한 법률(법률)(제20415호)(20260327).pdf 처리 완료.\t소요 시간 : 12.58 초\n",
      "대한민국과아메리카합중국간의상호방위조약제4조에의한시설과구역및대한민국에서의합중국군대의지위에관한협정의시행에관한민사특별법시행령(대통령령)(제27960호)(20170330).pdf 처리 완료.\t소요 시간 : 9.21 초\n",
      "핵융합에너지 개발진흥법 시행령(대통령령)(제28210호)(20170726).pdf 처리 완료.\t소요 시간 : 10.61 초\n",
      "공익신탁법 시행령(대통령령)(제26147호)(20150319).pdf 처리 완료.\t소요 시간 : 11.36 초\n",
      "농ㆍ축산ㆍ임ㆍ어업용 기자재 및 석유류에 대한 부가가치세 영세율 및 면세 적용 등에 관한 특례규정 시행규칙(기획재정부령)(제01056호)(20240401).pdf 처리 완료.\t소요 시간 : 10.18 초\n",
      "국가교육위원회 설치 및 운영에 관한 법률 시행령(대통령령)(제32627호)(20220721).pdf 처리 완료.\t소요 시간 : 12.10 초\n",
      "고엽제후유의증 등 환자지원 및 단체설립에 관한 법률 시행규칙(국가보훈부령)(제00035호)(20240814).pdf 처리 완료.\t소요 시간 : 14.01 초\n",
      "물가안정에 관한 법률(법률)(제20409호)(20240326).pdf 처리 완료.\t소요 시간 : 11.83 초\n",
      "농어촌정비법 시행규칙(해양수산부령)(제00680호)(20250104).pdf 처리 완료.\t소요 시간 : 17.32 초\n",
      "국가통합교통체계효율화법 시행령(대통령령)(제34731호)(20240731).pdf 처리 완료.\t소요 시간 : 26.65 초\n",
      "도시형소공인 지원에 관한 특별법(법률)(제19503호)(20231221).pdf 처리 완료.\t소요 시간 : 13.19 초\n",
      "노후준비 지원법(법률)(제19959호)(20240710).pdf 처리 완료.\t소요 시간 : 12.20 초\n",
      "부동산투자회사법 시행령(대통령령)(제34835호)(20240821).pdf 처리 완료.\t소요 시간 : 19.55 초\n",
      "대한민국근무기장령(대통령령)(제17945호)(20030325).pdf 처리 완료.\t소요 시간 : 10.28 초\n",
      "청소년활동 진흥법 시행령(대통령령)(제34442호)(20240423).pdf 처리 완료.\t소요 시간 : 13.60 초\n",
      "먹는물관리법 시행규칙(환경부령)(제01098호)(20240618).pdf 처리 완료.\t소요 시간 : 17.77 초\n",
      "영산강ㆍ섬진강수계 물관리 및 주민지원 등에 관한 법률 시행규칙(환경부령)(제01098호)(20240618).pdf 처리 완료.\t소요 시간 : 13.94 초\n",
      "평생교육법 시행령(대통령령)(제34406호)(20240419).pdf 처리 완료.\t소요 시간 : 20.00 초\n",
      "112신고의 운영 및 처리에 관한 법률(법률)(제19870호)(20240703).pdf 처리 완료.\t소요 시간 : 10.28 초\n",
      "아동수당법(법률)(제19455호)(20230914).pdf 처리 완료.\t소요 시간 : 11.82 초\n",
      "지방자치법 시행령(대통령령)(제34657호)(20240710).pdf 처리 완료.\t소요 시간 : 21.37 초\n",
      "공무원임용시험령(대통령령)(제33657호)(20250101).pdf 처리 완료.\t소요 시간 : 19.71 초\n",
      "교통약자의 이동편의 증진법(법률)(제19674호)(20250217).pdf 처리 완료.\t소요 시간 : 22.07 초\n",
      "만화진흥에 관한 법률 시행령(대통령령)(제33714호)(20230922).pdf 처리 완료.\t소요 시간 : 13.82 초\n",
      "국방과학기술혁신 촉진법 시행규칙(국방부령)(제01049호)(20210401).pdf 처리 완료.\t소요 시간 : 12.75 초\n",
      "신문 등의 진흥에 관한 법률 시행령(대통령령)(제33913호)(20231212).pdf 처리 완료.\t소요 시간 : 14.13 초\n",
      "국토교통과학기술 육성법 시행령(대통령령)(제34809호)(20240807).pdf 처리 완료.\t소요 시간 : 13.50 초\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모든 파일 처리 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(f\"{file_name} 파일 처리 중...\")\u001b[39;00m\n\u001b[1;32m     66\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 67\u001b[0m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 처리 완료.\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m소요 시간 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 초\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path, model_name, persist_directory)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, persist_directory: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformerEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     loader \u001b[38;5;241m=\u001b[39m PyMuPDFLoader(file_path)\n\u001b[1;32m     31\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddings.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:294\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    285\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    288\u001b[0m     model_name_or_path,\n\u001b[1;32m    289\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    293\u001b[0m ):\n\u001b[0;32m--> 294\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    307\u001b[0m         model_name_or_path,\n\u001b[1;32m    308\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    316\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:1655\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m         module_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1655\u001b[0m         module_path \u001b[38;5;241m=\u001b[39m \u001b[43mload_dir_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(module_path)\n\u001b[1;32m   1665\u001b[0m modules[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/sentence_transformers/util.py:1363\u001b[0m, in \u001b[0;36mload_dir_path\u001b[0;34m(model_name_or_path, directory, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;66;03m# Try to download from the remote\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1363\u001b[0m     repo_path \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;66;03m# Otherwise, try local (i.e. cache) only\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m     download_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py:169\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# if we have internet connection we want to list files to download\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     api \u001b[38;5;241m=\u001b[39m HfApi(\n\u001b[1;32m    163\u001b[0m         library_name\u001b[38;5;241m=\u001b[39mlibrary_name,\n\u001b[1;32m    164\u001b[0m         library_version\u001b[38;5;241m=\u001b[39mlibrary_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m     repo_info \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mProxyError):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Actually raise for those subclasses of ConnectionError\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/hf_api.py:2588\u001b[0m, in \u001b[0;36mHfApi.repo_info\u001b[0;34m(self, repo_id, revision, repo_type, timeout, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported repo type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/huggingface_hub/hf_api.py:2372\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[1;32m   2371\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[0;32m-> 2372\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   2374\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dl/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import time\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = self.model.encode(texts, convert_to_tensor=True, device=device)\n",
    "        return embeddings.cpu().tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "\n",
    "def process_file(file_path: str, model_name: str, persist_directory: str):\n",
    "    embedding_model = SentenceTransformerEmbeddings(model_name)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=lambda text: len(embedding_model.tokenizer.encode(text)),\n",
    "    )\n",
    "\n",
    "    docs_split = splitter.split_documents(documents)\n",
    "    # print(f\"문서 분할 완료: {len(docs_split)}개의 청크 생성\")\n",
    "\n",
    "    client = Chroma.from_documents(docs_split, embedding_model, persist_directory=persist_directory)\n",
    "    client.persist()\n",
    "    # print(\"벡터 추가 및 저장 완료\")\n",
    "\n",
    "    # 메모리 관리 - 리소스 해제\n",
    "    del embedding_model, loader, documents, docs_split, client\n",
    "\n",
    "    # MPS를 사용하는 경우 (M1 Mac)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    # 가비지 컬렉션 강제 실행\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "def main():\n",
    "    directory_path = 'file/현행법령/'\n",
    "    persist_directory = \"file/chroma_storage/multilingual_law/\"\n",
    "    model_name = \"jinaai/jina-embeddings-v3\"\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            # print(f\"{file_name} 파일 처리 중...\")\n",
    "            start_time = time.time()\n",
    "            process_file(file_path, model_name, persist_directory)\n",
    "            end_time = time.time()\n",
    "            print(f\"{file_name} 처리 완료.\\t소요 시간 : {end_time - start_time:.2f} 초\")\n",
    "\n",
    "    print(\"모든 파일 처리 완료.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
